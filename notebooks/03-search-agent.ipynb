{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "import json\n",
    "from langsmith import traceable\n",
    "\n",
    "# Load the configuration from the config.json file\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Set the state variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config[\"AWS_ACCESS_KEY_ID\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = config[\"OPENAI_API_SECRET_KEY\"]\n",
    "\n",
    "import os\n",
    "\n",
    "# Usage\n",
    "\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "openai_api_secret_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI # Using OpenAI for simplicity, replace with Anthropic if needed.\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Wikipedia search tool\n",
    "@tool\n",
    "def wikipedia_search(query: str):\n",
    "    \"\"\"Searches Wikipedia for a given query and returns a summary.\"\"\"\n",
    "    try:\n",
    "        url = f\"https://en.wikipedia.org/wiki/{query.replace(' ', '_')}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        content = soup.find('div', class_='mw-parser-output')\n",
    "        if content:\n",
    "            paragraphs = content.find_all('p')\n",
    "            summary = \" \".join(paragraph.text for paragraph in paragraphs[:3]).strip()\n",
    "            return summary\n",
    "        else:\n",
    "            return \"Error: Content not found on Wikipedia.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: Request failed - {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: An unexpected error occurred - {e}\"\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    # Check for tool calls in the *last response*, not the user's message.\n",
    "    # if len(messages) > 1 and messages[-1].tool_calls:  # Check the AI's response\n",
    "    #     return \"tools\"  # Go to tools node if the model suggests a tool call\n",
    "    if len(messages) > 1 and \"Error\" not in messages[-1].content and len(messages[-1].content) > 100:\n",
    "        return END  # Stop if a long enough, error-free summary is obtained\n",
    "    else:\n",
    "        return \"tools\" \n",
    "\n",
    "@traceable\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "\n",
    "# The overall state of the graph (this is the public state shared across nodes)\n",
    "class OverallState(BaseModel):\n",
    "    cv : str\n",
    "    summary : str\n",
    "    main_skills : str\n",
    "    pros : str\n",
    "    cons : str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.ai.model import model, call_model\n",
    "from src.ai.workflow import FlowState\n",
    "\n",
    "def agentic_search(initial_state: FlowState):\n",
    "    workflow = StateGraph(FlowState)\n",
    "\n",
    "    workflow.add_node(\"agent\", call_model)\n",
    "    tool_node = ToolNode([wikipedia_search])\n",
    "    workflow.add_node(\"tools\", tool_node)  # Update as necessary to ensure it aligns with the rest of your infrastructure\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "    workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    # checkpointer = MemorySaver()\n",
    "\n",
    "    app = workflow.compile()\n",
    "    # Run the agent with OpenAI\n",
    "    final_state = app.invoke(\n",
    "        initial_state)\n",
    "\n",
    "    print(final_state[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ai.utils import load_outputs_placeholder, load_prompts, add_task_to_prompts, get_initial_state\n",
    "\n",
    "storyline = \"love\"\n",
    "flow_type = \"search\"\n",
    "task = \"Summarize the history of Python\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Summarize the history of Python'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m initial_state_outputs_placeholder \u001b[38;5;241m=\u001b[39m load_outputs_placeholder(storyline, flow_type)\n\u001b[0;32m----> 4\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m \u001b[43mget_initial_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstoryline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AgenticLoveStory/src/ai/utils.py:29\u001b[0m, in \u001b[0;36mget_initial_state\u001b[0;34m(storyline, flow_type, task)\u001b[0m\n\u001b[1;32m     26\u001b[0m state_prompts \u001b[38;5;241m=\u001b[39m load_prompts(flow_type \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/state\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m task_prompts \u001b[38;5;241m=\u001b[39m load_prompts(flow_type \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/task\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m tasked_state_prompts \u001b[38;5;241m=\u001b[39m \u001b[43madd_task_to_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m FlowState(prompts\u001b[38;5;241m=\u001b[39mtasked_state_prompts, outputs\u001b[38;5;241m=\u001b[39minitial_state_outputs_placeholder)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m initial_state\n",
      "File \u001b[0;32m~/AgenticLoveStory/src/ai/prompts.py:18\u001b[0m, in \u001b[0;36madd_task_to_prompts\u001b[0;34m(state_prompts, task_prompts, task)\u001b[0m\n\u001b[1;32m     14\u001b[0m tasked_state_prompts \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (key, prompt) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mlist\u001b[39m(state_prompts\u001b[38;5;241m.\u001b[39mitems())):\n\u001b[0;32m---> 18\u001b[0m     tasked_prompt \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m+\u001b[39m \u001b[43mtask_prompts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m     tasked_state_prompts[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPart \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tasked_prompt\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tasked_state_prompts\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Summarize the history of Python'"
     ]
    }
   ],
   "source": [
    "initial_state_outputs_placeholder = load_outputs_placeholder(storyline, flow_type)\n",
    "\n",
    "\n",
    "initial_state = get_initial_state(storyline, flow_type, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = FlowState(prompts=tasked_state_prompts, outputs=initial_state_outputs_placeholder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magentic_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m, in \u001b[0;36magentic_search\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m app \u001b[38;5;241m=\u001b[39m workflow\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Run the agent with OpenAI\u001b[39;00m\n\u001b[1;32m     18\u001b[0m final_state \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mFlowState\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSummarize the history of Python\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "agentic_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph()\n",
    "\n",
    "workflow.add_node('agent', call_model)\n",
    "tool_node = ToolNode([wikipedia_search])\n",
    "workflow.add_node('tool', tool_node)\n",
    "workflow.add_node(END)\n",
    "\n",
    "workflow.add_edge(START, 'agent')\n",
    "workflow.add_conditional_edges('agent', should_continue)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer = checkpointer)\n",
    "\n",
    "final_state = app.invoke({\"messages\": [HumanMessage(content=\"Summarize the history of Python\")]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI # Using OpenAI for simplicity, replace with Anthropic if needed.\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
