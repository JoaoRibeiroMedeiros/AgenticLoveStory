{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "import json\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "class MetaDataGrader(BaseModel):\n",
    "    binary_score: str = Field(description=\" \")\n",
    "\n",
    "# Load the configuration from the config.json file\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Set the state variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config[\"AWS_ACCESS_KEY_ID\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "\n",
    "# Usage\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_TRACING_V2=True\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGCHAIN_API_KEY=\"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\"\n",
    "LANGCHAIN_PROJECT=\"Agentic-Love\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BedrockLLM' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangsmith\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traceable\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Auto-trace LLM calls in-context\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m bedrock_llm \u001b[38;5;241m=\u001b[39m \u001b[43mwrap_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBedrockLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta.llama3-70b-instruct-v1:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43maws_access_key_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_access_key_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43maws_secret_access_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_secret_access_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_kwargs= {\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9}\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Create a prompt template\u001b[39;00m\n\u001b[1;32m     20\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     21\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     22\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhilosophize on this: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/AgenticLoveStory/.env/lib/python3.9/site-packages/langsmith/wrappers/_openai.py:286\u001b[0m, in \u001b[0;36mwrap_openai\u001b[0;34m(client, tracing_extra, chat_name, completions_name)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_openai\u001b[39m(\n\u001b[1;32m    264\u001b[0m     client: C,\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m     completions_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    269\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m C:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Patch the OpenAI client to make it traceable.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate \u001b[38;5;241m=\u001b[39m _get_wrapper(  \u001b[38;5;66;03m# type: ignore[method-assign]\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m         \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate,\n\u001b[1;32m    287\u001b[0m         chat_name,\n\u001b[1;32m    288\u001b[0m         _reduce_chat,\n\u001b[1;32m    289\u001b[0m         tracing_extra\u001b[38;5;241m=\u001b[39mtracing_extra,\n\u001b[1;32m    290\u001b[0m         invocation_params_fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(_infer_invocation_params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    291\u001b[0m         process_outputs\u001b[38;5;241m=\u001b[39m_process_chat_completion,\n\u001b[1;32m    292\u001b[0m     )\n\u001b[1;32m    293\u001b[0m     client\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate \u001b[38;5;241m=\u001b[39m _get_wrapper(  \u001b[38;5;66;03m# type: ignore[method-assign]\u001b[39;00m\n\u001b[1;32m    294\u001b[0m         client\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate,\n\u001b[1;32m    295\u001b[0m         completions_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m         invocation_params_fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(_infer_invocation_params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "File \u001b[0;32m~/AgenticLoveStory/.env/lib/python3.9/site-packages/pydantic/main.py:896\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 896\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BedrockLLM' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "\n",
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "\n",
    "# Auto-trace LLM calls in-context\n",
    "\n",
    "bedrock_llm = wrap_openai(BedrockLLM(\n",
    "    model_id=\"meta.llama3-70b-instruct-v1:0\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key, \n",
    "    temperature=0.5,\n",
    "    max_tokens = 512,\n",
    "    # model_kwargs= {\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9}\n",
    "))\n",
    "\n",
    "# Create a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"Philosophize on this: {input}\"\n",
    ")\n",
    "\n",
    "# Create the LangChain LLM Chain\n",
    "transfer_news_chain = bedrock_llm | prompt_template\n",
    "\n",
    "\n",
    "# # Use the chain\n",
    "# input_text = \"What is chess?\"\n",
    "# result = transfer_news_chain.invoke(input_text)\n",
    "# result.to_messages()\n",
    "\n",
    "# Assuming result is a string that corresponds to binary_score\n",
    "# graded_result = MetaDataGrader(binary_score=result.to_string())\n",
    "# print(graded_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangsmith\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traceable\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Auto-trace LLM calls in-context\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m client \u001b[38;5;241m=\u001b[39m wrap_openai(\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "File \u001b[0;32m~/AgenticLoveStory/.env/lib/python3.9/site-packages/openai/_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI # Using OpenAI for simplicity, replace with Anthropic if needed.\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Wikipedia search tool\n",
    "@tool\n",
    "def wikipedia_search(query: str):\n",
    "    \"\"\"Searches Wikipedia for a given query and returns a summary.\"\"\"\n",
    "    try:\n",
    "        url = f\"https://en.wikipedia.org/wiki/{query.replace(' ', '_')}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        content = soup.find('div', class_='mw-parser-output')\n",
    "        if content:\n",
    "            paragraphs = content.find_all('p')\n",
    "            summary = \" \".join(paragraph.text for paragraph in paragraphs[:3]).strip()\n",
    "            return summary\n",
    "        else:\n",
    "            return \"Error: Content not found on Wikipedia.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: Request failed - {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: An unexpected error occurred - {e}\"\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    # Check for tool calls in the *last response*, not the user's message.\n",
    "    # if len(messages) > 1 and messages[-1].tool_calls:  # Check the AI's response\n",
    "    #     return \"tools\"  # Go to tools node if the model suggests a tool call\n",
    "    if len(messages) > 1 and \"Error\" not in messages[-1].content and len(messages[-1].content) > 100:\n",
    "        return END  # Stop if a long enough, error-free summary is obtained\n",
    "    else:\n",
    "        return \"tools\" \n",
    "\n",
    "@traceable\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode([wikipedia_search])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Binding tools to BedrockLLM failed: 'BedrockLLM' object has no attribute 'bind_tools'.  Tool usage might not function as expected. Attempting to continue without tool binding.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bedrock_llm = BedrockLLM(\n",
    "    model_id=\"meta.llama3-70b-instruct-v1:0\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key, \n",
    "    temperature=0.5,\n",
    "    max_tokens = 512,\n",
    "    # model_kwargs= {\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9}\n",
    ")\n",
    "\n",
    "#Attempt to bind tools (This might not work directly with BedrockLLM)\n",
    "try:\n",
    "    model = bedrock_llm.bind_tools(tools)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Binding tools to BedrockLLM failed: {e}.  Tool usage might not function as expected. Attempting to continue without tool binding.\")\n",
    "    model = bedrock_llm #If tool binding fails, proceed without it\n",
    "\n",
    "# ... (should_continue, call_model, workflow, checkpointer, app remain the same) ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " programming language.\n",
      "\n",
      "Python was created in the late ed by Guido van Rossum at the National Research Institute for Mathematics and Computer Science in the Netherlands. Here's a brief summary of the history of Python:\n",
      "\n",
      "* **1991:** Guido van Rossum begins working on Python at the National Research Institute for Mathematics and Computer Science in the Netherlands.\n",
      "* **1994:** Python 1.0 is released, featuring a simple syntax and a focus on code readability.\n",
      "* **1995:** Python 1.2 is released, adding support for functional programming and the `lambda` function.\n",
      "* **1996:** Python 1.3 is released, introducing the `with` statement and improved support for object-oriented programming.\n",
      "* **1999:** Python 1.5 is released, featuring a new garbage collector and improved performance.\n",
      "* **2000:** Python 1.6 is released, adding support for Unicode and improved support for Windows.\n",
      "* **2001:** Python 2.0 is released, featuring a new garbage collector and improved performance.\n",
      "* **2001-2005:** Python 2.1, 2.2, 2.3, and 2.4 are released, adding new features and improving performance.\n",
      "* **2005:** Python 2.5 is released, featuring a new `with` statement and improved support for asynchronous I/O.\n",
      "* **2008:** Python 2.6 and 3.0 are released, with Python 3.0 introducing significant changes to the language, including a new way of handling integer division and a revamped standard library.\n",
      "* **2009-2015:** Python 2.7, 3.1, 3.2, 3.3, and 3.4 are released, adding new features and improving performance.\n",
      "* **2015:** Python 3.5 is released, featuring a new `async` and `await` syntax for asynchronous programming.\n",
      "* **2016-2020:** Python 3.6, 3.7, 3.8, and 3.9 are released, adding new features and improving performance.\n",
      "* **2020:** Python 3.10 is released, featuring a new `match` statement for pattern matching.\n",
      "\n",
      "Throughout its history, Python has evolved to become a popular and versatile language, widely used in web development, scientific computing, data analysis, artificial intelligence, and more.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "#Run the agent\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Summarize the history of Python\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}} # Added thread_id\n",
    ")\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
