{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "import json\n",
    "from langsmith import traceable\n",
    "\n",
    "# Load the configuration from the config.json file\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Set the state variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config[\"AWS_ACCESS_KEY_ID\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = config[\"OPENAI_API_SECRET_KEY\"]\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# from uuid import uuid4\n",
    "# unique_id = uuid4().hex[0:8]\n",
    "\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"agentic-love\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"  # Update to your API key\n",
    "\n",
    "# Usage\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "openai_api_secret_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"image_desc\"],\n",
    "    template=\"Generate a prompt to generate an image based on the following description, make sure to keep the prompt length under 300: {image_desc}\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "gen_prompt = chain.run(\"halloween night at a haunted museum\")\n",
    "\n",
    "image_url = DallEAPIWrapper().run(gen_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Imagine a spooky Halloween night at a haunted museum. The moon casts an eerie glow over the old building as you approach the entrance. You can hear the faint sound of creepy music and cackling laughter coming from within. As you enter, the dimly lit halls are filled with ghoulish artifacts and shadowy figures. Suddenly, a ghostly apparition appears before you, sending chills down your spine. Capture this bone-chilling scene in an image and share with us.\" '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-NH5BMmrd8biLKbGRg4O7kplg/user-7laZfpcgRl5Ob4tgwKFoOC7H/img-RsKHq8Vc7ywu5qSfapdmOffU.png?st=2024-12-02T21%3A55%3A34Z&se=2024-12-02T23%3A55%3A34Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-12-02T08%3A04%3A58Z&ske=2024-12-03T08%3A04%3A58Z&sks=b&skv=2024-08-04&sig=dXqMzcinTkuUec%2B9s/XaZNybaeX6QKKR1DGqgZ7fnLw%3D'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI # Using OpenAI for simplicity, replace with Anthropic if needed.\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Wikipedia search tool\n",
    "@tool\n",
    "def wikipedia_search(query: str):\n",
    "    \"\"\"Searches Wikipedia for a given query and returns a summary.\"\"\"\n",
    "    try:\n",
    "        url = f\"https://en.wikipedia.org/wiki/{query.replace(' ', '_')}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        content = soup.find('div', class_='mw-parser-output')\n",
    "        if content:\n",
    "            paragraphs = content.find_all('p')\n",
    "            summary = \" \".join(paragraph.text for paragraph in paragraphs[:3]).strip()\n",
    "            return summary\n",
    "        else:\n",
    "            return \"Error: Content not found on Wikipedia.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: Request failed - {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: An unexpected error occurred - {e}\"\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    # Check for tool calls in the *last response*, not the user's message.\n",
    "    # if len(messages) > 1 and messages[-1].tool_calls:  # Check the AI's response\n",
    "    #     return \"tools\"  # Go to tools node if the model suggests a tool call\n",
    "    if len(messages) > 1 and \"Error\" not in messages[-1].content and len(messages[-1].content) > 100:\n",
    "        return END  # Stop if a long enough, error-free summary is obtained\n",
    "    else:\n",
    "        return \"tools\" \n",
    "\n",
    "@traceable\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Binding tools to BedrockLLM failed: .  Tool usage might not function as expected. Attempting to continue without tool binding.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "\n",
    "# The overall state of the graph (this is the public state shared across nodes)\n",
    "class OverallState(BaseModel):\n",
    "    cv : str\n",
    "    summary : str\n",
    "    main_skills : str\n",
    "    pros : str\n",
    "    cons : str\n",
    "\n",
    "def call_model(field, model):\n",
    "\n",
    "    response = model.invoke(field)\n",
    "\n",
    "    if isinstance(response, AIMessage):\n",
    "        response_content = response.content  # Adjust this based on the actual property\n",
    "    else:\n",
    "        response_content = str(response)\n",
    "    \n",
    "    # Return the response content as a string in OverallState\n",
    "    return response_content\n",
    "\n",
    "\n",
    "def make_summary(state: OverallState):\n",
    "\n",
    "    summary_prompt = \"\"\"Make a summary of the following CV, \n",
    "                        make sure to cover all skillset displayed in the candidates professional history, \n",
    "                        including leadership, engineering and others.\"\"\"\n",
    "\n",
    "    summary = call_model(summary_prompt + state.cv, model)\n",
    "    output_state = OverallState(cv = state.cv, summary = summary, main_skills = \"\", pros = \"\", cons = \"\")\n",
    "\n",
    "    return output_state\n",
    "\n",
    "def get_main_skills(state: OverallState):\n",
    "\n",
    "    get_main_skills_prompt = \"\"\" \\n\\n Identifying the main skills associated with this candidate. \n",
    "                            Notice that main skills, will be consistently displayed in his professional history. \n",
    "                            Also identify any rare skills that can differentiate this candidate from others.\\n\\n \"\"\"\n",
    "    leverage_summary_prompt = \"\"\" \\n\\n Use the summary you provided as a reference for this task as well: \\n\\n.\"\"\"\n",
    "   \n",
    "    main_skills = call_model(get_main_skills_prompt + state.cv + leverage_summary_prompt + state.cv, model)\n",
    "    output_state = OverallState(cv = state.cv, summary = state.summary, main_skills = main_skills, pros = \"\", cons = \"\")\n",
    "    \n",
    "    return output_state\n",
    "\n",
    "def get_pros(state: OverallState):\n",
    "\n",
    "    get_pros_prompt = \"Identify the most positive points in this candidate's CV\"\n",
    "    leverage_summary_prompt = \"\"\" \\n\\n Use the summary you provided as a reference for this task as well: \\n\\n\"\"\"\n",
    "    leverage_main_skills_prompt = \"\"\" \\n\\n Use the main skills you identified as a reference for this task as well: \\n\\n\"\"\"\n",
    "    \n",
    "    pros = call_model(get_pros_prompt + state.cv + leverage_summary_prompt + state.cv + leverage_main_skills_prompt + state.main_skills, model)\n",
    "    output_state = OverallState(cv = state.cv, summary = state.summary, main_skills = state.main_skills, pros = pros, cons = \"\")\n",
    "    \n",
    "    return output_state\n",
    "\n",
    "def get_cons(state: OverallState):\n",
    "\n",
    "    get_cons_prompt = \"Identify the negative points in this candidate's CV. Those points should be referenced as opportunities for improving.\\n\\n\"\n",
    "    leverage_summary_prompt = \"\"\" \\n\\n Use the summary you provided as a reference for this task as well: \\n\\n\"\"\"\n",
    "    leverage_main_skills_prompt = \"\"\" \\n\\n Use the main skills you identified as a reference for this task as well, what is lacking? \\n\\n\"\"\"\n",
    "    leverage_pros_prompt = \"\"\" \\n\\n Use the pros you identified as a reference for this task as well, what is lacking here? \\n\\n\"\"\"\n",
    "    \n",
    "    cons = call_model(get_cons_prompt + state.cv + leverage_summary_prompt + state.summary + leverage_pros_prompt + state.pros +  leverage_main_skills_prompt + state.main_skills, model)\n",
    "    output_state = OverallState(cv = state.cv, summary = state.summary, main_skills = state.main_skills, pros = state.pros, cons = cons)\n",
    "    \n",
    "    return output_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python, a high-level programming language, was created by Guido van Rossum and first released in 1991. It was designed with an emphasis on code readability, and its syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java.\n",
      "\n",
      "The language was named after the BBC show \"Monty Python's Flying Circus\" as Van Rossum was a fan of the show and wanted a short, unique, and slightly mysterious name for his invention.\n",
      "\n",
      "Python 2.0 was released in 2000, which included new features like garbage collection and Unicode support. In 2008, Python 3.0 was released, which was a major revision of the language that is not completely backward-compatible. However, Python 2 was still widely used for several years, before it was officially discontinued in 2020.\n",
      "\n",
      "Python has seen a surge in popularity in recent years, largely because of its use in a wide range of application domains, including web and internet development, scientific and numeric computing, education, and software development. Its simplicity and elegance have made it a popular choice for beginners and experts alike.\n",
      "\n",
      "Python's development is conducted largely through the Python Enhancement Proposal (PEP) process. The Python Software Foundation, a non-profit organization, manages and directs resources for Python and CPython development. Today, Python is maintained by a core development team in the community, and Guido van Rossum, the creator, remained involved in the decision-making process until his retirement in 2018.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bind tools if necessary (Note: OpenAI's direct integration may not support 'bind_tools' if this function is specific to other libraries)\n",
    "model = openai_llm  # In case you don’t need tool binding, you can directly use the model\n",
    "\n",
    "# Modify workflow node to use OpenAI model\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "\n",
    "tool_node = ToolNode([wikipedia_search])\n",
    "workflow.add_node(\"tools\", tool_node)  # Update as necessary to ensure it aligns with the rest of your infrastructure\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Run the agent with OpenAI\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Summarize the history of Python\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}  # Ensure this configuration is supported\n",
    ")\n",
    "\n",
    "print(final_state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph()\n",
    "\n",
    "workflow.add_node('agent', call_model)\n",
    "tool_node = ToolNode([wikipedia_search])\n",
    "workflow.add_node('tool', tool_node)\n",
    "workflow.add_node(END)\n",
    "\n",
    "workflow.add_edge(START, 'agent')\n",
    "workflow.add_conditional_edges('agent', should_continue)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer = checkpointer)\n",
    "\n",
    "final_state = app.invoke({\"messages\": [HumanMessage(content=\"Summarize the history of Python\")]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI # Using OpenAI for simplicity, replace with Anthropic if needed.\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
